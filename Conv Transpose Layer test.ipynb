{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad82391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe69387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===weight shape===  torch.Size([3, 4, 5])\n",
      "===output shape===  torch.Size([6, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.ConvTranspose1d(in_channels = 3, out_channels = 4, kernel_size = 5, bias = False)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "input = torch.rand(6, 3, 7)\n",
    "print(F\"===output shape===  {layer(input).shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02997df8",
   "metadata": {},
   "source": [
    "The first 6 from output shape is the same 6 from the input shape, which is the batch_size and consists through this calc.\n",
    "The 4 from output shape is the same as out_channels param when creating the layer.\n",
    "the last 11 from output shapde is calculated as 7(from input) + 5(kernel_size from layer) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317a6c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1., 2., 4.]]])\n",
      "===weight shape===  torch.Size([1, 1, 2])\n",
      "===weight===  Parameter containing:\n",
      "tensor([[[1.0000, 0.1100]]], requires_grad=True)\n",
      "===result===  tensor([[[1.0000, 2.1100, 4.2200, 0.4400]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test 1, basic without bias.\n",
    "input = torch.tensor([1, 2, 4], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.11], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c162c8d",
   "metadata": {},
   "source": [
    "ConvTranspose1d:\n",
    "input:      1         2         4\n",
    "weight:   1  0.11   1  0.11   1  0.11  \n",
    "        1     0.11+2     0.22+4    0.44           \n",
    "result: 1      2.11       4.22     0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e910f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1., 1.]]])\n",
      "===weight shape===  torch.Size([1, 1, 2])\n",
      "===weight===  Parameter containing:\n",
      "tensor([[[1., 1.]]], requires_grad=True)\n",
      "===bias shape===  torch.Size([1])\n",
      "===bias===  Parameter containing:\n",
      "tensor([100.], requires_grad=True)\n",
      "===result===  tensor([[[101., 102., 101.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test 2, bias.\n",
    "input = torch.tensor([1, 1], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1, 1], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "print(F\"===bias shape===  {layer.bias.shape}\")\n",
    "layer.bias = torch.nn.parameter.Parameter(torch.tensor([100], dtype = torch.float32, requires_grad = True))\n",
    "print(F\"===bias===  {layer.bias}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd1eb22c",
   "metadata": {},
   "source": [
    "input:      1       1\n",
    "weight:   1   1   1   1\n",
    "        1      1+1      1\n",
    "        1       2       1\n",
    "bias:  100     100     100     \n",
    "result:101     102     101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24853529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1., 2.]]])\n",
      "===weight shape===  torch.Size([1, 1, 2])\n",
      "===weight===  Parameter containing:\n",
      "tensor([[[1.0000, 0.1100]]], requires_grad=True)\n",
      "===result===  tensor([[[1.0000, 0.1100, 2.0000, 0.2200]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test 3, stride\n",
    "input = torch.tensor([1, 2], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False, stride = 2)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.11], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9254d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1., 2., 3.]]])\n",
      "===weight shape===  torch.Size([1, 1, 2])\n",
      "===weight===  Parameter containing:\n",
      "tensor([[[1.0000, 0.1100]]], requires_grad=True)\n",
      "===result===  tensor([[[2.1100, 3.2200]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test 4, padding\n",
    "input = torch.tensor([1, 2, 3], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False, padding = 1)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.11], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6f1903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1.],\n",
      "         [2.],\n",
      "         [3.],\n",
      "         [4.]]])\n",
      "===weight shape===  torch.Size([4, 1, 2])\n",
      "===weight===  Parameter containing:\n",
      "tensor([[[1.0000e+00, 1.0000e-01]],\n",
      "\n",
      "        [[1.0000e-02, 1.0000e-03]],\n",
      "\n",
      "        [[1.0000e-04, 1.0000e-05]],\n",
      "\n",
      "        [[1.0000e-06, 1.0000e-07]]], requires_grad=True)\n",
      "===result===  tensor([[[1.0200e+00, 1.0200e-01],\n",
      "         [3.0400e-04, 3.0400e-05]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test 5, groups. I don't know how this would help in any means. But it's there. \n",
    "input = torch.tensor([1, 2, 3, 4], dtype = torch.float32).view(1, 4, 1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 4, out_channels = 2, kernel_size = 2, bias = False, groups = 2)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.1, 0.01, 0.001, 1e-4, 1e-5, 1e-6, 1e-7], dtype = torch.float32, requires_grad = True).view(*layer.weight.shape))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa3afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===input===  tensor([[[1., 2., 3.]]])\n",
      "===weight shape===  torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\YAGAOD~1\\AppData\\Local\\Temp/ipykernel_14120/2176052728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvTranspose1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mF\"===weight shape===  {layer.weight.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mF\"===weight===  {layer.weight}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "#test 6, dilation.\n",
    "input = torch.tensor([1, 2, 3], dtype = torch.float32).view(1, 1, 3)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False, dilation = 2)\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.1], dtype = torch.float32, requires_grad = True).view(*l.weight.shape))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74685124",
   "metadata": {},
   "source": [
    "      1           2           3\n",
    "    1 0.1       1 0.1       1 0.1   \n",
    "  1    0.1    2    0.2    3    0.3  \n",
    "  1           2        \"0.1+3\"      0.2       0.3\n",
    "  1           2          3.1        0.2       0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test , output_padding case 1, output_padding < stride\n",
    "input = torch.tensor([1, 2], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "layer = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False, output_padding = 1, stride = 2 )\n",
    "print(F\"===weight shape===  {layer.weight.shape}\")\n",
    "layer.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.1], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {layer.weight}\")\n",
    "\n",
    "print(F\"===result===  {layer(input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865336e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test , output_padding case 1, output_padding < stride\n",
    "input = torch.tensor([1, 2], dtype = torch.float32).view(1, 1, -1)\n",
    "print(F\"===input===  {input}\")\n",
    "\n",
    "l = torch.nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 2, bias = False, output_padding = 1, dilation = 2 )\n",
    "print(F\"===weight shape===  {l.weight.shape}\")\n",
    "l.weight = torch.nn.parameter.Parameter(torch.tensor([1., 0.1], dtype = torch.float32, requires_grad = True).view(1, 1, -1))\n",
    "print(F\"===weight===  {l.weight}\")\n",
    "\n",
    "print(F\"===result===  {l(input)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec07968f",
   "metadata": {},
   "source": [
    "padding_mode is not shown here. I didn't find the doc. Probably padding_mode works only for conv, but doesn't for conv transpose. For more info, check out the other file for Conv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
