{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825c7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f15ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "tensor([[-1.2247,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 1.2247,  0.0000]])\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "tensor([[-1.2247,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 1.2247,  0.0000]])\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]])\n",
      "weight: None\n",
      "bias:   None\n"
     ]
    }
   ],
   "source": [
    "#Case 1, affine = False. In this case, this layer hebaviors statelessly.\n",
    "layer = torch.nn.BatchNorm1d(2, momentum=0.1, affine=False, track_running_stats=False).train()\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[2, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [2, 0], [3, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print()\n",
    "layer.eval()\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[2, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [2, 0], [3, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(F\"weight: {layer.weight}\")\n",
    "print(F\"bias:   {layer.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd45b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[-1.2247,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 1.2247,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[-1.2247,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 1.2247,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[ 1.0000,  0.0000],\n",
      "        [-1.0000,  0.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "weight: Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "bias:   Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Case 1, affine = True. In this case, this layer still hebaviors statelessly.\n",
    "layer = torch.nn.BatchNorm1d(2, momentum=0.1, track_running_stats=False).train()#affine = True as default\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[2, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [2, 0], [3, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print()\n",
    "layer.eval()\n",
    "#print(layer.training)#False\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[2, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [2, 0], [3, 0]], dtype = torch.float32)))\n",
    "print(layer(torch.tensor([[1, 0], [0, 0]], dtype = torch.float32)))\n",
    "print(F\"weight: {layer.weight}\")\n",
    "print(F\"bias:   {layer.bias}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7740d812",
   "metadata": {},
   "source": [
    "I don't know if I messed anything up in the test. The BN with affine seems to behavior like a pure stateless BN followed by a element-wise multiply layer and then element-wise addition layer. The learnable parts are the 2 non BN layers behind. \n",
    "The interesting thing is that even the BN layer is set to evaluation mode, it still relies on batch(batch_size >= 2) and doesn't give out stable hebavior when the input data changes. I don't know why, but if this is the case, I personally recommend people use BN as another version of softmax. If softmax helps, BN may also help. If softmax doesn't, BN may neither. I didn't test, it's only intuition.\n",
    "But even BN, softmax and sigmoid(tanh) partly destroy the concrete value, but they protect the gradient and also keep the relation between element in hidden layers. This feature probably helps for classification, but I don't believe it help regretion, according to my intuition.\n",
    "Anyway, the test ends here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
