{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687d3b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import numpy\n",
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\")\n",
    "# A True is gonna be printed\n",
    "#dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e67b9a",
   "metadata": {},
   "source": [
    "#####This cell is copied from internet.\n",
    "#####But I better doc is here https://pytorch.org/docs/stable/notes/extending.html\n",
    "\n",
    "class RoundNoGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        print(F\"forward {x}\")\n",
    "        return x.round()\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        print(F\"backward {g}\")\n",
    "        return g \n",
    "\n",
    "\n",
    "l = torch.nn.L1Loss()\n",
    "input = torch.autograd.Variable(torch.randn(3), requires_grad=True)\n",
    "x = RoundNoGradient.apply(input)\n",
    "print(F\"x {x}\")\n",
    "y = x.mean()\n",
    "#x = input.round()\n",
    "y.backward()\n",
    "\n",
    "#output = l(x, torch.autograd.Variable(torch.randn(3)))\n",
    "#output.backward()\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6fd13a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.6667, 0.0000, 0.0000])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YAGAOD~1\\AppData\\Local\\Temp/ipykernel_12016/3980839213.py:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = r*0\n"
     ]
    }
   ],
   "source": [
    "class SafeMul(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        with torch.no_grad():\n",
    "            r = (x*y).numpy()\n",
    "        #-1\n",
    "        temp = r*0\n",
    "        temp2 = temp != 0.\n",
    "        ctx.save_for_backward(x, y, torch.tensor(temp2))\n",
    "        \n",
    "        for i, f in enumerate(temp2):\n",
    "            if f:\n",
    "                r[i] = 0\n",
    "        #-2\n",
    "        return torch.tensor(r, requires_grad = True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        x, y, flags = ctx.saved_tensors\n",
    "        #print(F\"Inside backward g {g}\")\n",
    "        #print(F\"Inside backward x {x}\")\n",
    "        #print(F\"Inside backward y {y}\")\n",
    "        #print(F\"flags {flags}\")\n",
    "        \n",
    "        for i, f in enumerate(flags):\n",
    "            #print(i,f)\n",
    "            if f:\n",
    "                #print(i,f,123)\n",
    "                g[(i)] = 0.\n",
    "        #-2\n",
    "        temp = x*0\n",
    "        temp2 = temp != 0.\n",
    "        for i, f in enumerate(temp2):\n",
    "            if f:\n",
    "                x[i] = 0.\n",
    "        #-2\n",
    "        temp = y*0\n",
    "        temp2 = temp != 0.\n",
    "        for i, f in enumerate(temp2):\n",
    "            if f:\n",
    "                y[i] = 0.\n",
    "        #-2\n",
    "        #print(F\"Inside backward x {x}\")\n",
    "        #print(F\"Inside backward y {y}\")\n",
    "        #print(F\"Inside backward g {g}\")\n",
    "        return g*y, g*x\n",
    "\n",
    "    \n",
    "x = torch.tensor([3., float(\"nan\"), 100e100], requires_grad = True)\n",
    "#print(F\"x {x}\")\n",
    "#x = torch.tensor([1., 1., 1.], requires_grad = True)\n",
    "#print(x)\n",
    "y = torch.tensor([11., 111., 1111.], requires_grad = True)\n",
    "#print(F\"y {y}\")\n",
    "r = SafeMul.apply(x, y).mean()\n",
    "#r = layer.forward(x, y).mean()\n",
    "\n",
    "r.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5372f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])#.numpy()\n",
    "a[1] = 4\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4ac121-372f-4beb-9a28-ccaa12687c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([ 0.9000,  1.1000,  1.9000,  2.1000,  3.9000,  4.1000, -0.9000, -1.1000,\n",
      "        -1.9000, -2.1000, -3.9000, -4.1000], grad_fn=<CatBackward0>)\n",
      "output before:tensor([1.0000, 0.9000, 0.1000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000, 0.1000,\n",
      "        0.0000, 0.0000, 0.0000], grad_fn=<LLC2_functional_partBackward>)\n",
      "output after:1.0\n",
      "grad of a: tensor([0.0000, 0.2750, 0.4750, 0.0000, 0.0000, 0.0102, 0.0000, 0.2750, 0.4750,\n",
      "        0.0000, 0.0000, 0.0102])\n",
      "grad of b: tensor([0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0025, 0.0000, 0.2500, 0.2500,\n",
      "        0.0000, 0.0000, 0.0025])\n",
      "grad of input: None\n"
     ]
    }
   ],
   "source": [
    "'''This part is actually something I made for myself in real pytorch projects.\n",
    "This one shows how to implement stateful, fully customized backward propagation.\n",
    "Notice, the first class inherits from torch.autograd.Function\n",
    "Also Notice, how the backward function in the first class returns 5 values.\n",
    "But this example is probably not perfect. \"with torch.no_grad():\" could probably help.\n",
    "Idk, I may try this detail in the future.\n",
    "'''\n",
    "import torch\n",
    "\n",
    "class LLC2_functional_part(torch.autograd.Function):\n",
    "    '''Linear leaky clamp 2\n",
    "    The forward and backward hebavior differently.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a, b, factor, negative_slope):\n",
    "        #assert factor.item()>=1.\n",
    "        #assert negative_slope.item()>0. and negative_slope.item()<1.\n",
    "        ctx.save_for_backward(x, a, b, torch.tensor([factor]), torch.tensor([negative_slope]))\n",
    "        x = torch.abs(x)\n",
    "        x = a * x + b\n",
    "        # Then, double relu to make the result almost inside 0 to 1\n",
    "        x = torch.relu(x)\n",
    "        x = 1 - torch.relu(1 - x)\n",
    "        return x\n",
    "        pass\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        x, a, b, factor, negative_slope = ctx.saved_tensors\n",
    "        factor = factor.item()\n",
    "        negative_slope = negative_slope.item()\n",
    "        boundary = torch.abs(b/a)*factor\n",
    "        x_abs = torch.abs(x)\n",
    "        out_flags = x_abs>boundary\n",
    "        ori_ax_b = a * torch.abs(x) + b\n",
    "        in_flags = (ori_ax_b < 1.) * (ori_ax_b > 0.)\n",
    "        flags = in_flags + negative_slope * out_flags\n",
    "        b_grad = g*flags\n",
    "        a_grad = g*flags*x_abs\n",
    "        x_less_than_0 = x<0.\n",
    "        x_less_than_0 = x_less_than_0 * -2 + 1\n",
    "        x_grad = g*flags*a*x_less_than_0\n",
    "        return x_grad, a_grad, b_grad, None, None\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return F'LLC2_functional. It\\'s designed only for the LLC2 layer. It behaviors differently in forward propagation and backward propagation.'\n",
    "        pass\n",
    "    pass\n",
    "class LLC2(torch.nn.Module):\n",
    "    '''Linear leaky clamp version 2.0\n",
    "    The forward pass is pretty simple.\n",
    "    '''\n",
    "    def __init__(self, size, * , a = -1., b = 2., boundary_factor = 2., negative_slope=0.01):\n",
    "        super(LLC2, self).__init__()\n",
    "        self.a = torch.nn.Parameter(torch.full((size, ), a, dtype = torch.float32), True)\n",
    "        self.b = torch.nn.Parameter(torch.full((size, ), b, dtype = torch.float32), True)\n",
    "        assert boundary_factor>=1.\n",
    "        self.boundary_factor = boundary_factor\n",
    "        self.negative_slope = negative_slope\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        self.a = torch.nn.Parameter(-torch.abs(self.a))\n",
    "        x = LLC2_functional_part.apply(x, self.a, self.b, self.boundary_factor, self.negative_slope)\n",
    "        # The parameters for the specific function are: x, a, b, factor, negative_slope\n",
    "        return x\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return F'Linear leaky clamp ver 2.0. a: {self.a}  b: {self.b}  the boundary: {torch.abs(self.b/self.a)} boundary_factor: {self.boundary_factor:.1f}  negative_slope: {self.negative_slope:.4f}'\n",
    "    pass\n",
    "\n",
    "input = torch.tensor([0.9,1.1,1.9,2.1,3.9,4.1], requires_grad = True)\n",
    "input = torch.cat((input, input*-1))\n",
    "#input = input.view(1, -1) # Uncomment this line to check if this layer works for (batch_size, dimention).\n",
    "print(F\"input:{input}\")\n",
    "l = LLC2(input.shape[0])\n",
    "output = l(input)\n",
    "print(F\"output before:{output}\")\n",
    "output = output.mean()*3\n",
    "#output = output/output.item()\n",
    "output.backward()\n",
    "print(F\"output after:{output}\")\n",
    "print(F\"grad of a: {l.a.grad}\")\n",
    "print(F\"grad of b: {l.b.grad}\")\n",
    "print(F\"grad of input: {input.grad}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
